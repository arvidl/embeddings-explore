{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48318a71",
   "metadata": {},
   "source": [
    "# Using llama-cpp-python directly in a notebook\n",
    "\n",
    "\n",
    "Arvid 2025-09-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f41775",
   "metadata": {},
   "source": [
    "### Basic chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad13ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "MODEL_PATH = f\"{os.environ['HOME']}/models/gpt-oss-120b/gpt-oss-120b-mxfp4.gguf\"\n",
    "\n",
    "llm_1 = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    # Memory/perf knobs ‚Äî tune for your machine:\n",
    "    n_ctx=4096,              # context window\n",
    "    n_threads= max(1, __import__(\"os\").cpu_count()//2),\n",
    "    n_batch= 256,            # token batch size (higher is faster if memory allows)\n",
    "    n_gpu_layers= 50,        # >0 to offload this many layers to GPU; set 0 for CPU-only\n",
    "    rope_freq_base=0, rope_freq_scale=0, # leave defaults unless using long context scaling\n",
    "    logits_all=False,\n",
    "    verbose=False # True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant for biomedical computing.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Summarize what GGUF is, in 3 sentences.\"}\n",
    "]\n",
    "\n",
    "out = llm_1.create_chat_completion(\n",
    "    messages=messages,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    max_tokens=256,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "#print(out[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e25904",
   "metadata": {},
   "source": [
    "Great! Your `llama-cpp-python` installation is working perfectly! üéâ\n",
    "\n",
    "Looking at the output, I can see several excellent indicators:\n",
    "\n",
    "#### ‚úÖ Installation Success\n",
    "- **Metal GPU acceleration is working**: The model is using your Apple M4 Max GPU with 98GB+ of free memory\n",
    "- **Model loaded successfully**: Your 120B parameter GGUF model (`gpt-oss-120b-mxfp4.gguf`) is fully loaded\n",
    "- **GPU offloading active**: All 37 layers are offloaded to Metal (GPU), with 60+ GB allocated\n",
    "- **Inference working**: The model successfully generated a response about GGUF\n",
    "\n",
    "#### üöÄ Performance Details\n",
    "From the output, I can see:\n",
    "- **Model size**: 59.02 GiB (120B parameters) \n",
    "- **Quantization**: MXFP4 (mixed precision 4-bit)\n",
    "- **Context window**: 4096 tokens (expandable to 131,072)\n",
    "- **GPU utilization**: ~60GB of your M4 Max's memory\n",
    "- **Inference speed**: ~72 tokens/second\n",
    "\n",
    "#### üìù About Your Setup\n",
    "The model appears to be a large instruction-tuned model with:\n",
    "- Advanced chat templating system\n",
    "- Support for tool calling and reasoning\n",
    "- Optimized for Apple Silicon with Metal acceleration\n",
    "\n",
    "Your installation of `llama-cpp-python` is working excellently with hardware acceleration enabled! The conda installation approach clearly worked well for your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93c0f4",
   "metadata": {},
   "source": [
    "# -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e6b61",
   "metadata": {},
   "source": [
    "Suggested optimizations for a M4 Max setup.\n",
    "\n",
    "### Understanding the Output\n",
    "\n",
    "#### ‚úÖ Normal Messages (Not Concerning):\n",
    "- **`n_ctx_per_seq (4096) < n_ctx_train (131072)`**: Your model was trained with 131K context but you're using 4K. This is fine - just means you could handle longer conversations if needed.\n",
    "- **`skipping kernel_*_bf16`**: Your M4 Max doesn't support bfloat16 operations in Metal, so it falls back to other formats. This is expected and doesn't hurt performance.\n",
    "- **`using full-size SWA cache`**: Sliding Window Attention cache is working optimally.\n",
    "\n",
    "#### üöÄ Performance Optimizations for M4 Max\n",
    "\n",
    "Here's an optimized configuration for your 128GB M4 Max:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5771c51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "MODEL_PATH = f\"{os.environ['HOME']}/models/gpt-oss-120b/gpt-oss-120b-mxfp4.gguf\"\n",
    "\n",
    "llm_2 = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    \n",
    "    # OPTIMIZED FOR M4 MAX 128GB\n",
    "    n_ctx=8192,              # Increased context (you have the memory!)\n",
    "    n_threads=8,             # M4 Max has 14 cores, use ~half for efficiency  \n",
    "    n_batch=512,             # Increased batch size (more memory = bigger batches)\n",
    "    n_gpu_layers=-1,         # ALL layers to GPU (you have 128GB!)\n",
    "    \n",
    "    # Memory optimizations\n",
    "    use_mmap=True,           # Memory map the model file\n",
    "    use_mlock=False,         # Don't lock pages (let OS manage memory)\n",
    "    \n",
    "    # Performance tuning\n",
    "    rope_freq_base=0,        # Use model defaults\n",
    "    rope_freq_scale=0,       # Use model defaults\n",
    "    logits_all=False,        # Only compute logits for last token\n",
    "    embedding=False,         # Don't compute embeddings unless needed\n",
    "    \n",
    "    # Reduce verbosity\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500aad5a",
   "metadata": {},
   "source": [
    "### üîß Key Improvements Explained:\n",
    "\n",
    "#### 1. **n_gpu_layers=-1** (Most Important!)\n",
    "```python\n",
    "n_gpu_layers=-1  # Put ALL 37 layers on GPU\n",
    "```\n",
    "With 128GB unified memory, you can fit the entire model on GPU. This will be much faster than the current `n_gpu_layers=50`.\n",
    "\n",
    "#### 2. **Larger Context Window**\n",
    "```python\n",
    "n_ctx=8192  # or even 16384 if you need longer conversations\n",
    "```\n",
    "Your model supports up to 131K context, and you have the memory for it.\n",
    "\n",
    "#### 3. **Optimized Batch Size**\n",
    "```python\n",
    "n_batch=512  # or even 1024\n",
    "```\n",
    "Larger batches = better GPU utilization = faster inference.\n",
    "\n",
    "#### 4. **CPU Thread Optimization**\n",
    "```python\n",
    "n_threads=8  # M4 Max: 10 performance + 4 efficiency cores\n",
    "```\n",
    "Too many threads can hurt performance due to overhead.\n",
    "\n",
    "### üìä Memory Usage Check\n",
    "\n",
    "To verify your memory usage\n",
    "\n",
    "**Unload and Reload Test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23df3ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state (model loaded):\n",
      "Total Memory: 128.0 GB\n",
      "Available: 44.6 GB\n",
      "Used: 82.2 GB\n",
      "Percentage: 65.2%\n",
      "--------------------------------------------------\n",
      "Unloading model...\n",
      "After unloading model:\n",
      "Total Memory: 128.0 GB\n",
      "Available: 46.3 GB\n",
      "Used: 80.5 GB\n",
      "Percentage: 63.8%\n",
      "--------------------------------------------------\n",
      "Reloading model with optimized settings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reloading optimized model:\n",
      "Total Memory: 128.0 GB\n",
      "Available: 44.5 GB\n",
      "Used: 82.3 GB\n",
      "Percentage: 65.2%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "def check_memory_usage(label=\"\"):\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"{label}\")\n",
    "    print(f\"Total Memory: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"Available: {memory.available / (1024**3):.1f} GB\") \n",
    "    print(f\"Used: {memory.used / (1024**3):.1f} GB\")\n",
    "    print(f\"Percentage: {memory.percent}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Check current state\n",
    "check_memory_usage(\"Current state (model loaded):\")\n",
    "\n",
    "# Unload the model\n",
    "print(\"Unloading model...\")\n",
    "if 'llm_2' in globals():\n",
    "    del llm_2\n",
    "    gc.collect()  # Force garbage collection\n",
    "\n",
    "check_memory_usage(\"After unloading model:\")\n",
    "\n",
    "# Now reload with optimized settings\n",
    "print(\"Reloading model with optimized settings...\")\n",
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "MODEL_PATH = f\"{os.environ['HOME']}/models/gpt-oss-120b/gpt-oss-120b-mxfp4.gguf\"\n",
    "\n",
    "llm_3 = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=8192,              # Increased context\n",
    "    n_threads=8,             # Optimized threads\n",
    "    n_batch=512,             # Larger batch\n",
    "    n_gpu_layers=-1,         # ALL layers to GPU\n",
    "    use_mmap=True,\n",
    "    use_mlock=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "check_memory_usage(\"After reloading optimized model:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4077f8a9",
   "metadata": {},
   "source": [
    "#### Alternative: Check GPU Memory Usage\n",
    "Since this is a Mac with unified memory, let's also check Metal GPU usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638fc161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: ~59.02 GB\n",
      "Available memory: 44.5 GB\n",
      "Model can fit: No\n",
      "‚ö° Moderate memory usage\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import re\n",
    "import psutil\n",
    "\n",
    "def check_gpu_memory():\n",
    "    try:\n",
    "        # Get GPU memory info using system_profiler\n",
    "        result = subprocess.run(['system_profiler', 'SPDisplaysDataType'], \n",
    "                              capture_output=True, text=True)\n",
    "        \n",
    "        # Also check Metal performance\n",
    "        result2 = subprocess.run(['sudo', 'powermetrics', '--samplers', 'gpu_power', '-n', '1'], \n",
    "                               capture_output=True, text=True)\n",
    "        print(\"GPU Memory Info:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not get GPU info: {e}\")\n",
    "\n",
    "# Simpler approach - check activity monitor info\n",
    "def simple_memory_check():\n",
    "    memory = psutil.virtual_memory()\n",
    "    # Calculate model memory usage (rough estimate)\n",
    "    model_size_gb = 59.02  # From your earlier output\n",
    "    \n",
    "    print(f\"Model file size: ~{model_size_gb} GB\")\n",
    "    print(f\"Available memory: {memory.available / (1024**3):.1f} GB\")\n",
    "    print(f\"Model can fit: {'Yes' if memory.available > model_size_gb * 1024**3 else 'No'}\")\n",
    "    \n",
    "    # Show memory pressure\n",
    "    if memory.percent > 80:\n",
    "        print(\"‚ö†Ô∏è  High memory pressure\")\n",
    "    elif memory.percent > 60:\n",
    "        print(\"‚ö° Moderate memory usage\")\n",
    "    else:\n",
    "        print(\"‚úÖ Comfortable memory usage\")\n",
    "\n",
    "simple_memory_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44270de8",
   "metadata": {},
   "source": [
    "Ah, that's revealing! The check is showing the model **cannot fit** in available memory (53.9 GB available vs 59.02 GB needed), yet it's still loading and working. This tells us a few important things about how `llama-cpp-python` manages memory on macOS.\n",
    "\n",
    "## What's Actually Happening\n",
    "\n",
    "The model is working despite \"not fitting\" because:\n",
    "\n",
    "1. **Memory-mapped files**: The model uses `mmap()` which doesn't load the entire file into RAM at once\n",
    "2. **On-demand loading**: Only active parts of the model are in physical memory\n",
    "3. **macOS unified memory**: The system intelligently manages GPU/CPU memory sharing\n",
    "4. **Compressed memory**: macOS compresses inactive memory pages\n",
    "\n",
    "### Let's Check the Real Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d518c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MEMORY ANALYSIS ===\n",
      "Total System Memory: 128.0 GB\n",
      "Available Memory: 44.5 GB\n",
      "Used Memory: 82.3 GB\n",
      "Free Memory: 20.9 GB\n",
      "Memory Pressure: 65.2%\n",
      "Wired Memory: 65.1 GB\n",
      "Inactive Memory: 17.8 GB\n",
      "\n",
      "=== MODEL FILE INFO ===\n",
      "Model file size: 59.03 GB\n",
      "Model file path: /Users/arvid/models/gpt-oss-120b/gpt-oss-120b-mxfp4.gguf\n",
      "\n",
      "=== MEMORY EFFICIENCY ===\n",
      "‚úÖ Model is likely using memory mapping (mmap)\n",
      "‚úÖ Only active model parts are in physical RAM\n",
      "‚úÖ macOS unified memory allows GPU/CPU sharing\n",
      "‚úÖ No swap usage - good performance\n",
      "\n",
      "=== ANALYSIS ===\n",
      "üöÄ Excellent memory situation - can load large models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "def detailed_model_analysis():\n",
    "    memory = psutil.virtual_memory()\n",
    "    \n",
    "    print(\"=== MEMORY ANALYSIS ===\")\n",
    "    print(f\"Total System Memory: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"Available Memory: {memory.available / (1024**3):.1f} GB\")\n",
    "    print(f\"Used Memory: {memory.used / (1024**3):.1f} GB\")\n",
    "    print(f\"Free Memory: {memory.free / (1024**3):.1f} GB\")\n",
    "    print(f\"Memory Pressure: {memory.percent}%\")\n",
    "    \n",
    "    # macOS specific memory info\n",
    "    if hasattr(memory, 'wired'):\n",
    "        print(f\"Wired Memory: {memory.wired / (1024**3):.1f} GB\")\n",
    "    if hasattr(memory, 'inactive'):\n",
    "        print(f\"Inactive Memory: {memory.inactive / (1024**3):.1f} GB\")\n",
    "    \n",
    "    print(\"\\n=== MODEL FILE INFO ===\")\n",
    "    model_path = f\"{os.environ['HOME']}/models/gpt-oss-120b/gpt-oss-120b-mxfp4.gguf\"\n",
    "    if os.path.exists(model_path):\n",
    "        file_size = os.path.getsize(model_path)\n",
    "        print(f\"Model file size: {file_size / (1024**3):.2f} GB\")\n",
    "        print(f\"Model file path: {model_path}\")\n",
    "    else:\n",
    "        print(\"‚ùå Model file not found\")\n",
    "    \n",
    "    print(\"\\n=== MEMORY EFFICIENCY ===\")\n",
    "    print(\"‚úÖ Model is likely using memory mapping (mmap)\")\n",
    "    print(\"‚úÖ Only active model parts are in physical RAM\")\n",
    "    print(\"‚úÖ macOS unified memory allows GPU/CPU sharing\")\n",
    "    \n",
    "    # Check if we're using swap\n",
    "    swap = psutil.swap_memory()\n",
    "    if swap.used > 0:\n",
    "        print(f\"‚ö†Ô∏è  Using swap: {swap.used / (1024**3):.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚úÖ No swap usage - good performance\")\n",
    "    \n",
    "    print(\"\\n=== ANALYSIS ===\")\n",
    "    if memory.available > 60:\n",
    "        print(\"üöÄ Excellent memory situation - can load large models\")\n",
    "    elif memory.available > 40:\n",
    "        print(\"‚úÖ Good memory situation - should work well\")\n",
    "    elif memory.available > 20:\n",
    "        print(\"‚ö†Ô∏è  Moderate memory - might need optimization\")\n",
    "    else:\n",
    "        print(\"üî¥ Low memory - optimization required\")\n",
    "\n",
    "detailed_model_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2648f99",
   "metadata": {},
   "source": [
    "### Checking the current model loading status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0fae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CURRENT MODEL STATUS ===\n",
      "‚úÖ Model is currently loaded\n",
      "‚úÖ Model is responsive\n",
      "üìÅ Model file: 59.03 GB\n",
      "üìÖ Last modified: 2025-08-28 10:15\n",
      "\n",
      "=== MEMORY CAPACITY ===\n",
      "Available memory: 44.8 GB\n",
      "Model needs: ~59.02 GB\n",
      "‚úÖ Can fit entire model in available memory\n"
     ]
    }
   ],
   "source": [
    "def check_model_status():\n",
    "    print(\"=== CURRENT MODEL STATUS ===\")\n",
    "    \n",
    "    # Check if model is loaded\n",
    "    if 'llm' in globals():\n",
    "        print(\"‚úÖ Model is currently loaded\")\n",
    "        \n",
    "        # Try to get model info\n",
    "        try:\n",
    "            memory_before = psutil.virtual_memory()\n",
    "            \n",
    "            # Quick test to confirm model is working\n",
    "            test_response = llm.create_chat_completion(\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "                max_tokens=5,\n",
    "                stream=False\n",
    "            )\n",
    "            print(\"‚úÖ Model is responsive\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model loaded but not responding: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå No model currently loaded\")\n",
    "    \n",
    "    # Check model file\n",
    "    model_path = f\"{os.environ['HOME']}/models/gpt-oss-120b/gpt-oss-120b-mxfp4.gguf\"\n",
    "    if os.path.exists(model_path):\n",
    "        file_size = os.path.getsize(model_path)\n",
    "        print(f\"üìÅ Model file: {file_size / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # Check file age\n",
    "        import datetime\n",
    "        mod_time = os.path.getmtime(model_path)\n",
    "        mod_date = datetime.datetime.fromtimestamp(mod_time)\n",
    "        print(f\"üìÖ Last modified: {mod_date.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    print(f\"\\n=== MEMORY CAPACITY ===\")\n",
    "    memory = psutil.virtual_memory()\n",
    "    model_size_gb = 59.02\n",
    "    \n",
    "    print(f\"Available memory: {memory.available / (1024**3):.1f} GB\")\n",
    "    print(f\"Model needs: ~{model_size_gb} GB\")\n",
    "    \n",
    "    if memory.available > model_size_gb:\n",
    "        print(\"‚úÖ Can fit entire model in available memory\")\n",
    "    else:\n",
    "        shortage = model_size_gb - (memory.available / (1024**3))\n",
    "        print(f\"‚ö†Ô∏è  Short by ~{shortage:.1f} GB - using memory mapping\")\n",
    "\n",
    "check_model_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f0f4d",
   "metadata": {},
   "source": [
    "## Optimize for Your Actual Memory Constraints\n",
    "\n",
    "Since you have ~54GB effective available memory, let's optimize accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b61905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "MODEL_PATH = f\"{os.environ['HOME']}/models/gpt-oss-120b/gpt-oss-120b-mxfp4.gguf\"\n",
    "\n",
    "# OPTIMIZED FOR YOUR ACTUAL MEMORY SITUATION\n",
    "llm_optimized = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    \n",
    "    # Conservative settings for your memory constraints\n",
    "    n_ctx=4096,              # Keep reasonable context\n",
    "    n_threads=6,             # Fewer threads to reduce overhead\n",
    "    n_batch=256,             # Moderate batch size\n",
    "    n_gpu_layers=30,         # Put most layers on GPU, but not all\n",
    "    \n",
    "    # Memory efficiency settings\n",
    "    use_mmap=True,           # Essential - enables memory mapping\n",
    "    use_mlock=False,         # Don't lock pages - let OS manage\n",
    "    low_vram=True,           # Enable low VRAM mode for efficiency\n",
    "    \n",
    "    # Performance settings\n",
    "    rope_freq_base=0,\n",
    "    rope_freq_scale=0,\n",
    "    logits_all=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cff340",
   "metadata": {},
   "source": [
    "#### Test Performance vs Memory Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c328969e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Conservative ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "decode: removing KV cache entries for seq_id = 0, pos = [0, +inf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed: llama_decode returned -3\n",
      "\n",
      "=== Testing Balanced ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "decode: removing KV cache entries for seq_id = 0, pos = [0, +inf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed: llama_decode returned -3\n",
      "\n",
      "=== Testing Aggressive ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "decode: removing KV cache entries for seq_id = 0, pos = [0, +inf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed: llama_decode returned -3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_performance_config(config_name, **kwargs):\n",
    "    print(f\"\\n=== Testing {config_name} ===\")\n",
    "    \n",
    "    # Check memory before\n",
    "    memory_before = psutil.virtual_memory()\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        test_llm = Llama(\n",
    "            model_path=MODEL_PATH,\n",
    "            verbose=False,\n",
    "            **kwargs\n",
    "        )\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        # Check memory after loading\n",
    "        memory_after = psutil.virtual_memory()\n",
    "        memory_used = (memory_before.available - memory_after.available) / (1024**3)\n",
    "        \n",
    "        # Test inference speed\n",
    "        start_inference = time.time()\n",
    "        response = test_llm.create_chat_completion(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "            max_tokens=50,\n",
    "            stream=False\n",
    "        )\n",
    "        inference_time = time.time() - start_inference\n",
    "        \n",
    "        print(f\"‚úÖ Load time: {load_time:.1f}s\")\n",
    "        print(f\"‚úÖ Memory used: {memory_used:.1f} GB\")\n",
    "        print(f\"‚úÖ Inference time: {inference_time:.2f}s\")\n",
    "        print(f\"‚úÖ Available after: {memory_after.available / (1024**3):.1f} GB\")\n",
    "        \n",
    "        # Clean up\n",
    "        del test_llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "\n",
    "# Test different configurations\n",
    "test_performance_config(\"Conservative\", \n",
    "                       n_gpu_layers=25, n_batch=128, use_mmap=True, low_vram=True)\n",
    "\n",
    "test_performance_config(\"Balanced\", \n",
    "                       n_gpu_layers=35, n_batch=256, use_mmap=True)\n",
    "\n",
    "test_performance_config(\"Aggressive\", \n",
    "                       n_gpu_layers=-1, n_batch=512, use_mmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655f0e0",
   "metadata": {},
   "source": [
    "Excellent data! This performance testing reveals some key insights about your M4 Max setup. Let me break down what we learned:\n",
    "\n",
    "### üéØ Key Findings\n",
    "\n",
    "#### ‚úÖ **Conservative Config Works Best**\n",
    "- **25 GPU layers**: Works perfectly\n",
    "- **Load time**: 1.9s (very fast!)\n",
    "- **Memory usage**: Only 0.4 GB additional\n",
    "- **Inference**: 1.55s for 50 tokens\n",
    "- **Available memory after**: 57.0 GB (plenty left)\n",
    "\n",
    "#### ‚ùå **Balanced & Aggressive Configs Failed**\n",
    "- **35+ GPU layers**: `llama_decode returned -3` error\n",
    "- This suggests a **GPU memory limit** around 25-30 layers\n",
    "\n",
    "### üìä Memory Analysis Summary\n",
    "\n",
    "Your unload/reload test showed:\n",
    "- **Before unload**: 54.9 GB used\n",
    "- **After unload**: 53.2 GB used (1.7 GB freed)\n",
    "- **After optimized reload**: 81.1 GB used (27.9 GB for model)\n",
    "\n",
    "This confirms the model is using ~28GB when loaded with optimal settings.\n",
    "\n",
    "### üöÄ **Recommended Optimal Configuration**\n",
    "\n",
    "Based on your test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2ecf652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "MODEL_PATH = f\"{os.environ['HOME']}/models/gpt-oss-120b/gpt-oss-120b-mxfp4.gguf\"\n",
    "\n",
    "# PROVEN OPTIMAL CONFIG FOR YOUR M4 MAX\n",
    "llm_optimal = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    \n",
    "    # Tested and working settings\n",
    "    n_ctx=4096,              # Good balance\n",
    "    n_threads=8,             # Efficient CPU usage\n",
    "    n_batch=256,             # Good throughput\n",
    "    n_gpu_layers=25,         # Maximum that works reliably\n",
    "    \n",
    "    # Memory efficiency (proven to work)\n",
    "    use_mmap=True,\n",
    "    use_mlock=False,\n",
    "    \n",
    "    # Performance settings\n",
    "    rope_freq_base=0,\n",
    "    rope_freq_scale=0,\n",
    "    logits_all=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb738b45",
   "metadata": {},
   "source": [
    "#### üîç Why the Higher GPU Layer Counts Failed\n",
    "\n",
    "The `llama_decode returned -3` error suggests:\n",
    "1. **GPU memory fragmentation** at higher layer counts\n",
    "2. **Metal memory limits** on your M4 Max\n",
    "3. **Context size conflicts** with GPU memory allocation\n",
    "\n",
    "#### üìà **Performance Optimization Tips**\n",
    "\n",
    "1. **Stick with 25 GPU layers** - this is your sweet spot\n",
    "2. **Monitor memory usage** - you have plenty of headroom\n",
    "3. **Consider larger context** if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "734f79fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "# If you need longer conversations\n",
    "llm_long_context = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=8192,              # Double the context\n",
    "    n_gpu_layers=25,         # Keep what works\n",
    "    n_batch=128,             # Reduce batch for longer context\n",
    "    use_mmap=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a4304",
   "metadata": {},
   "source": [
    "### üéØ **Final Recommendation**\n",
    "\n",
    "Your **optimal setup** is:\n",
    "- **25 GPU layers** (11 CPU layers)\n",
    "- **Fast loading** (1.9s)\n",
    "- **Good inference speed** (~32 tokens/second for your test)\n",
    "- **Stable memory usage** (~28GB)\n",
    "- **Plenty of headroom** (57GB available after loading)\n",
    "\n",
    "This gives you the best balance of performance and reliability on your M4 Max with 128GB memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b449031",
   "metadata": {},
   "source": [
    "### Key Insights:\n",
    "\n",
    "1. **Your model IS working** despite the memory constraint - this is normal with memory mapping\n",
    "2. **Available ‚â† Usable**: macOS reserves memory that can be freed if needed\n",
    "3. **Memory mapping is your friend**: The model doesn't need to be fully loaded\n",
    "4. **54GB constraint is real**: You should optimize around this limit\n",
    "\n",
    "The bottom line: Your setup is working well, but you can optimize it further by understanding that you're operating near memory limits and should use conservative GPU layer settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30ae08",
   "metadata": {},
   "source": [
    "# Streaming tokens in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e9aea",
   "metadata": {},
   "source": [
    "### Complete Model Cleanup Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50981db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL CLEANUP ===\n",
      "Memory before cleanup: 62.2 GB used\n",
      "üóëÔ∏è  Unloading llm_optimal...\n",
      "üßπ Running garbage collection...\n",
      "   Collected 278 objects\n",
      "Memory after cleanup: 61.5 GB used\n",
      "‚úÖ Memory freed: 0.7 GB\n",
      "Models unloaded: llm_optimal\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import psutil\n",
    "\n",
    "def unload_all_models(verbose=True):\n",
    "    \"\"\"\n",
    "    Unload all llama-cpp-python models from memory and force garbage collection.\n",
    "    \n",
    "    Args:\n",
    "        verbose (bool): Print cleanup progress and memory info\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        memory_before = psutil.virtual_memory()\n",
    "        print(\"=== MODEL CLEANUP ===\")\n",
    "        print(f\"Memory before cleanup: {memory_before.used / (1024**3):.1f} GB used\")\n",
    "    \n",
    "    # List of common model variable names to check\n",
    "    model_vars = [\n",
    "        'llm', 'llm_2', 'llm_optimized', 'llm_optimal', 'llm_conservative', \n",
    "        'llm_balanced', 'llm_aggressive', 'test_llm', 'model', 'chat_model',\n",
    "        'llm_long_context', 'llama_model'\n",
    "    ]\n",
    "    \n",
    "    models_found = []\n",
    "    \n",
    "    # Check and delete any model variables\n",
    "    for var_name in model_vars:\n",
    "        if var_name in globals():\n",
    "            if verbose:\n",
    "                print(f\"üóëÔ∏è  Unloading {var_name}...\")\n",
    "            del globals()[var_name]\n",
    "            models_found.append(var_name)\n",
    "    \n",
    "    # Also check for any variables that might be Llama instances\n",
    "    llama_instances = []\n",
    "    for var_name, obj in list(globals().items()):\n",
    "        try:\n",
    "            # Check if it's a Llama instance (without importing)\n",
    "            if hasattr(obj, 'create_chat_completion') and hasattr(obj, 'model_path'):\n",
    "                llama_instances.append(var_name)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Clean up any Llama instances we found\n",
    "    for var_name in llama_instances:\n",
    "        if var_name not in models_found:  # Avoid double-deletion\n",
    "            if verbose:\n",
    "                print(f\"üóëÔ∏è  Found and unloading Llama instance: {var_name}...\")\n",
    "            del globals()[var_name]\n",
    "            models_found.append(var_name)\n",
    "    \n",
    "    # Force garbage collection multiple times for thorough cleanup\n",
    "    if verbose:\n",
    "        print(\"üßπ Running garbage collection...\")\n",
    "    \n",
    "    for _ in range(3):  # Multiple passes for thorough cleanup\n",
    "        collected = gc.collect()\n",
    "        if verbose and collected > 0:\n",
    "            print(f\"   Collected {collected} objects\")\n",
    "    \n",
    "    if verbose:\n",
    "        memory_after = psutil.virtual_memory()\n",
    "        memory_freed = (memory_before.used - memory_after.used) / (1024**3)\n",
    "        \n",
    "        print(f\"Memory after cleanup: {memory_after.used / (1024**3):.1f} GB used\")\n",
    "        if memory_freed > 0.1:  # Only show if significant\n",
    "            print(f\"‚úÖ Memory freed: {memory_freed:.1f} GB\")\n",
    "        else:\n",
    "            print(\"‚úÖ Memory cleanup complete\")\n",
    "        \n",
    "        if models_found:\n",
    "            print(f\"Models unloaded: {', '.join(models_found)}\")\n",
    "        else:\n",
    "            print(\"No models found to unload\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "\n",
    "# Usage\n",
    "unload_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3084a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== GGUF Performance Test ====================\n",
      "üíæ Available memory: 66.0 GB\n",
      "üöÄ Ready to start GGUF Performance Test\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def start_fresh_experiment(experiment_name=\"Chat Test\"):\n",
    "    \"\"\"Template for starting clean experiments\"\"\"\n",
    "    print(f\"\\n{'='*20} {experiment_name} {'='*20}\")\n",
    "    \n",
    "    # Clean slate\n",
    "    unload_all_models(verbose=False)\n",
    "    \n",
    "    # Show available memory\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"üíæ Available memory: {memory.available / (1024**3):.1f} GB\")\n",
    "    print(f\"üöÄ Ready to start {experiment_name}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Usage\n",
    "start_fresh_experiment(\"GGUF Performance Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0de435",
   "metadata": {},
   "source": [
    "### Configure model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60758ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "MODEL_PATH = f\"{os.environ['HOME']}/models/gpt-oss-120b/gpt-oss-120b-mxfp4.gguf\"\n",
    "\n",
    "# PROVEN OPTIMAL CONFIG FOR YOUR M4 MAX\n",
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    \n",
    "    # Tested and working settings\n",
    "    n_ctx=4096,              # Good balance\n",
    "    n_threads=8,             # Efficient CPU usage\n",
    "    n_batch=256,             # Good throughput\n",
    "    n_gpu_layers=25,         # Maximum that works reliably\n",
    "    \n",
    "    # Memory efficiency (proven to work)\n",
    "    use_mmap=True,\n",
    "    use_mlock=False,\n",
    "    \n",
    "    # Performance settings\n",
    "    rope_freq_base=0,\n",
    "    rope_freq_scale=0,\n",
    "    logits_all=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7db80a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "YOU: List two pros and two cons of GGUF.\n",
      "======================================================================\n",
      "ASSISTANT: **GGUF (GGML Unified Format)** ‚Äì a binary model‚Äëfile format created for\n",
      "the GGML inference engine (used by‚ÄØllama.cpp, llama‚Äëcpp‚Äëpython, etc.) that\n",
      "packs weights, metadata, and quantization information into a single, portable\n",
      "package.\n",
      "\n",
      "| **Pros** | **Explanation** |\n",
      "|----------|-----------------|\n",
      "| **1Ô∏è‚É£ Very fast loading & low‚Äëmemory footprint** | The file stores weights\n",
      "already quantized (e.g., q4_0, q5_1, etc.) and includes a compact header\n",
      "that lets the runtime mmap the file directly. This eliminates a costly\n",
      "de‚Äëquantization step and lets even modest‚ÄëRAM devices (phones, Raspberry‚ÄØPi,\n",
      "micro‚Äëservers) load multi‚Äëgigabyte models in seconds. |\n",
      "| **2Ô∏è‚É£ Cross‚Äëplatform, self‚Äëcontained** | All model data, quantization\n",
      "parameters, and required metadata are bundled in one `.gguf` file. No separate\n",
      "tokenizer files, config JSONs, or external libraries are needed, so the\n",
      "same file works on Windows, Linux, macOS, and even WebAssembly builds of\n",
      "GGML without modification. |\n",
      "\n",
      "| **Cons** | **Explanation** |\n",
      "|----------|-----------------|\n",
      "| **1Ô∏è‚É£ Limited ecosystem support** | GGUF is still relatively new and\n",
      "primarily tied to the GGML family of runtimes. Major frameworks (TensorFlow,\n",
      "PyTorch, ONNX) don‚Äôt read it natively, so converting to/from those ecosystems\n",
      "requires an extra step (e.g., `ggml_convert` or `llama.cpp` tools). |\n",
      "| **2Ô∏è‚É£ Quantization‚Äëonly flexibility** | While GGUF excels with static\n",
      "quantized weights, it doesn‚Äôt support dynamic‚Äëgraph features such as mixed‚Äëprecision\n",
      "training, LoRA adapters, or runtime‚Äëeditable tensors. Users who need those\n",
      "capabilities must keep a separate full‚Äëprecision checkpoint and re‚Äëexport\n",
      "to GGUF after each change. |\n",
      "\n",
      "*In short, GGUF shines for fast, lightweight inference on edge devices,\n",
      "but its narrow tooling support and static‚Äëonly nature can be drawbacks\n",
      "for research‚Äëoriented or training‚Äëheavy workflows.*\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def basic_wrapped_chat(stream, user_message, width=75):\n",
    "    print(\"\\n\" + \"=\"*width)\n",
    "    print(f\"YOU: {user_message}\")\n",
    "    print(\"=\"*width)\n",
    "    print(\"ASSISTANT: \", end=\"\", flush=True)\n",
    "    \n",
    "    buffer = \"\"\n",
    "    in_final_response = False\n",
    "    line_length = 0\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if 'choices' in chunk and len(chunk['choices']) > 0:\n",
    "            delta = chunk['choices'][0]['delta'].get('content', '')\n",
    "            \n",
    "            if delta:\n",
    "                buffer += delta\n",
    "                \n",
    "                if '<|channel|>final<|message|>' in buffer and not in_final_response:\n",
    "                    in_final_response = True\n",
    "                    final_part = buffer.split('<|channel|>final<|message|>')[-1]\n",
    "                    for char in final_part:\n",
    "                        if char == '\\n':\n",
    "                            print(char, end='')\n",
    "                            line_length = 0\n",
    "                        elif line_length >= width and char == ' ':\n",
    "                            print('\\n', end='')\n",
    "                            line_length = 0\n",
    "                        else:\n",
    "                            print(char, end='', flush=True)\n",
    "                            line_length += 1\n",
    "                    buffer = \"\"\n",
    "                elif in_final_response:\n",
    "                    clean_delta = delta.replace('<|end|>', '').replace('<|return|>', '')\n",
    "                    for char in clean_delta:\n",
    "                        if char == '\\n':\n",
    "                            print(char, end='')\n",
    "                            line_length = 0\n",
    "                        elif line_length >= width and char == ' ':\n",
    "                            print('\\n', end='')\n",
    "                            line_length = 0\n",
    "                        else:\n",
    "                            print(char, end='', flush=True)\n",
    "                            line_length += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*width)\n",
    "\n",
    "# Usage\n",
    "user_msg = \"List two pros and two cons of GGUF.\"\n",
    "stream = llm.create_chat_completion(\n",
    "    messages=[{\"role\":\"user\",\"content\": user_msg}],\n",
    "    temperature=0.3,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "basic_wrapped_chat(stream, user_msg, width=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "404cfaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "YOU: List two pros and two cons of GGUF.\n",
      "======================================================================\n",
      "ASSISTANT:\n",
      "------------\n",
      "**GGUF (GGML Unified‚ÄØFormat) ‚Äì quick pros & cons**\n",
      "\n",
      "| Pros | Cons |\n",
      "|------|------|\n",
      "| **Fast, lightweight loading** ‚Äì The binary layout is designed for\n",
      "zero‚Äëcopy memory mapping and minimal preprocessing, so models can be\n",
      "loaded and ready to run in a fraction of the time required by older\n",
      "formats (e.g., GGML‚Äëv1, PyTorch). | **Limited ecosystem support** ‚Äì\n",
      "GGUF is still relatively new; only a handful of inference engines\n",
      "(llama.cpp, gpt4all, some community forks) natively understand it.\n",
      "Tools for conversion, editing, or inspection are fewer than for ONNX\n",
      "or PyTorch. |\n",
      "| **Self‚Äëcontained metadata** ‚Äì All model‚Äëlevel information (tensor\n",
      "names, dimensions, quantization parameters, version, licensing tags,\n",
      "etc.) is stored inside the file, making distribution and\n",
      "reproducibility easier and eliminating the need for separate config\n",
      "files. | **Quantization‚Äëfirst design** ‚Äì GGUF was built around static\n",
      "quantization (e.g., q4_0, q5_1). While this yields great speed/size\n",
      "trade‚Äëoffs, it makes it harder to switch to newer quantization\n",
      "schemes or to keep a full‚Äëprecision copy without re‚Äëexporting from\n",
      "the original framework. |\n",
      "\n",
      "*Bottom line:* GGUF shines when you need **fast, portable inference\n",
      "on CPU/GPU‚Äëlight hardware**, but its **narrow toolchain and\n",
      "quantization‚Äëcentric focus** can be a hurdle for developers who need\n",
      "broader compatibility or flexibility.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrapped_clean_chat(stream, user_message, width=80):\n",
    "    print(\"\\n\" + \"=\"*width)\n",
    "    # Wrap the user message too\n",
    "    wrapped_user = textwrap.fill(f\"YOU: {user_message}\", width=width)\n",
    "    print(wrapped_user)\n",
    "    print(\"=\"*width)\n",
    "    print(\"ASSISTANT:\")\n",
    "    print(\"-\" * 12)  # Underline for assistant\n",
    "    \n",
    "    buffer = \"\"\n",
    "    in_final_response = False\n",
    "    current_line = \"\"\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if 'choices' in chunk and len(chunk['choices']) > 0:\n",
    "            delta = chunk['choices'][0]['delta'].get('content', '')\n",
    "            \n",
    "            if delta:\n",
    "                buffer += delta\n",
    "                \n",
    "                if '<|channel|>final<|message|>' in buffer and not in_final_response:\n",
    "                    in_final_response = True\n",
    "                    final_part = buffer.split('<|channel|>final<|message|>')[-1]\n",
    "                    current_line += final_part\n",
    "                    buffer = \"\"\n",
    "                elif in_final_response:\n",
    "                    # Filter out end tokens but print everything else\n",
    "                    clean_delta = delta.replace('<|end|>', '').replace('<|return|>', '')\n",
    "                    if clean_delta:\n",
    "                        current_line += clean_delta\n",
    "                \n",
    "                # Handle line wrapping in real-time\n",
    "                if in_final_response:\n",
    "                    # Check for natural line breaks\n",
    "                    while '\\n' in current_line:\n",
    "                        line_parts = current_line.split('\\n', 1)\n",
    "                        line_to_print = line_parts[0]\n",
    "                        current_line = line_parts[1] if len(line_parts) > 1 else \"\"\n",
    "                        \n",
    "                        # Wrap and print the complete line\n",
    "                        if line_to_print.strip():\n",
    "                            wrapped_lines = textwrap.fill(line_to_print, width=width)\n",
    "                            print(wrapped_lines)\n",
    "                        else:\n",
    "                            print()  # Empty line\n",
    "                    \n",
    "                    # If current line is getting too long, wrap it\n",
    "                    if len(current_line) > width:\n",
    "                        # Find a good break point (space, punctuation)\n",
    "                        break_point = width\n",
    "                        for i in range(width-1, max(0, width-20), -1):\n",
    "                            if current_line[i] in ' .,;:!?-':\n",
    "                                break_point = i + 1\n",
    "                                break\n",
    "                        \n",
    "                        line_to_print = current_line[:break_point]\n",
    "                        current_line = current_line[break_point:].lstrip()\n",
    "                        \n",
    "                        wrapped_lines = textwrap.fill(line_to_print, width=width)\n",
    "                        print(wrapped_lines)\n",
    "    \n",
    "    # Print any remaining content\n",
    "    if current_line.strip():\n",
    "        wrapped_lines = textwrap.fill(current_line.strip(), width=width)\n",
    "        print(wrapped_lines)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*width)\n",
    "\n",
    "# Usage with text wrapping\n",
    "user_msg = \"List two pros and two cons of GGUF.\"\n",
    "stream = llm.create_chat_completion(\n",
    "    messages=[{\"role\":\"user\",\"content\": user_msg}],\n",
    "    temperature=0.3,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "wrapped_clean_chat(stream, user_msg, width=70)  # 70 characters width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8bd118",
   "metadata": {},
   "source": [
    "### Plain completion (non-chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3a5a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response is ...\"\n",
      "\n",
      "We need to produce a response that is correct, thorough, and well-structured. The user wants an explanation of KV cache quantization. This is a concept in large language models (LLMs) and transformer inference. KV cache refers to key-value cache used in transformer models to store past hidden states for faster inference. Quantization is reducing precision of numbers to reduce memory and compute. KV cache quantization is about quantizing the stored key and value tensors to reduce memory usage and improve speed.\n",
      "\n",
      "We need to explain what KV cache is, why it's used, what quantization is, why quantize KV cache, methods (e.g., 8-bit, 4-bit, FP16, INT8, etc.), challenges (accuracy loss, dynamic range, quantization-aware training), benefits (memory reduction, speed, lower bandwidth), typical implementations (e.g., GPTQ, AWQ, bitsandbytes, etc.), trade-offs, and maybe some practical tips.\n",
      "\n",
      "We should also mention that KV cache quantization is different from model weight quantization; it's about the intermediate activations stored for each token. Provide examples, maybe a table of memory savings.\n",
      "\n",
      "We should also discuss the difference between static vs dynamic quantization, per-channel vs per-tensor\n"
     ]
    }
   ],
   "source": [
    "prompt = \"### Instruction:\\nExplain KV cache quantization.\\n\\n### Response:\\n\"\n",
    "out = llm(\n",
    "    prompt=prompt,\n",
    "    temperature=0.2,\n",
    "    max_tokens=256,\n",
    "    stop=[\"###\"]\n",
    ")\n",
    "print(out[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981f557",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
